---
title: "Untitled"
author: "Daniel HÃ¶hn"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

```{r load data}
# dir = "C:/Users/unisys/Documents/Studium/qAlgorithms/build/"
dir = "C:/Users/unisys/Documents/Studium/Masterarbeit/"
polarity = "_positive_"
name = "SP_DDA_P7"
# name = "warburg_testdata"
# name = "210229_C1_S1_W_MI_2_pos"
# name = "20240805_AA_DK_02_Ibu_Kali_0_20240808125236"
file = str_c(dir, name, polarity)
file_feat = str_c(file, "features.csv")
file_bins = str_c(file, "bins.csv")
file_featCens = str_c(file, "featCen.csv")
file_compRegs = str_c(file, "components.csv")

features = read_csv(file_feat)
featCens = read_csv(file_featCens)
compRegs = read_csv(file_compRegs)
```

I am relatively certain that the previous efforts with the normalised intensity profiles only seemed promising due to
the differences between absolute intensities being minimal. Scaling itself seems to be the biggest problem in solving
shape similarity through the tanimoto index / score 

```{r}
compID = 1
mzDiffCount = 0
intensityDiffGreaterTwo = 0

for (compID in 1:max(features$CompID)) {
  feat = features[which(features$CompID == compID),]
   points = featCens[which(featCens$featureID %in% feat$ID),]
  for (i in 1:length(feat$ID)) {
     points$mz[which(points$featureID == feat$ID[i])] = feat$mz[i]
  }
  print(
   ggplot(points)+
     geom_point(aes(retentionTime, area, colour = as.factor(mz)))+
     labs(caption = paste0("compID: ", compID))
   )

}
```


```{r}
compID = 247
mzDiffCount = 0
intensityDiffGreaterTwo = 0

for (compID in 1:max(features$CompID)) {
  feat = features[which(features$CompID == compID),]
   points = featCens[which(featCens$featureID %in% feat$ID),]
  for (i in 1:length(feat$ID)) {
     points$mz[which(points$featureID == feat$ID[i])] = feat$mz[i]
  }
  MZdiffs = c(outer(feat$mz, feat$mz, `-`))
  IntDiffs = c(outer(feat$area, feat$area, `-`))
  mzDiffCount[compID] = sum(abs(MZdiffs - 1.003355) < mean(feat$mzUncertainty))
  intensityDiffGreaterTwo[compID] = sum(IntDiffs > max(feat$area) / 2)
}
sum(mzDiffCount != 0)
sum(intensityDiffGreaterTwo != 0)
sel = which(mzDiffCount != 0)
for (compID in sel) {
  feat = features[which(features$CompID == compID),]
   points = featCens[which(featCens$featureID %in% feat$ID),]
   for (i in 1:length(feat$ID)) {
     points$mz[which(points$featureID == feat$ID[i])] = feat$mz[i]
   }

   print(
   ggplot(points)+
     geom_point(aes(retentionTime, height, colour = as.factor(mz)))+
     labs(caption = paste0("compID: ", compID))
   )
}

# check number of true duplicate retention times
RTmatch = 0
for (compID in 1:max(features$CompID)) {
  feat = features[which(features$CompID == compID),]
  points = featCens[which(featCens$featureID %in% feat$ID),]
  RTmatch[compID] = (length(points$mz) - length(unique(points$retentionTime))) / length(points$mz)
}
```

```{r plot distribution of components}
ggplot(features)+
  geom_boxplot(aes(retentionTime, CompID != 0))

# a very high portion of features in the last few minutes is in components
cdplot(as.factor(CompID != 0) ~ retentionTime, data = features)
plot(density(features$retentionTime))

feat = features[which(features$CompID == 343),]
   points = featCens[which(featCens$featureID %in% feat$ID),]
   for (i in 1:length(feat$ID)) {
     points$mz[which(points$featureID == feat$ID[i])] = feat$mz[i]
   }

   print(
   ggplot(points)+
     geom_point(aes(retentionTime, height, colour = as.factor(mz)))+
     labs(caption = paste0("compID: ", compID))
   )
```

```{r gini coeff}
# basic idea: take all points in the two or more same-length, interpolated vectors and normalise them to
# a vector length of 1. Then, take the standard deviation for every discrete x value. If all vectors follow
# the same profile with a normally distributed difference, the contribution of every point to the total error
# should be roughly equivalent. This corresponds to the cumulative error function x = y. The area of the 
# empirical cumulative error (triangle) divided by the ideal area yields a measure of equivalence that does
# not heavily weight the intensity uncertainty

features_sel = features[which(features$CompID == 702),]
comp_sel = compRegs[702,] # this must be -1 for 0-indexed arrays
points_A = featCens[which(featCens$featureID == features_sel$ID[1]),]
points_B = featCens[which(featCens$featureID == features_sel$ID[2]),]
points_C = featCens[which(featCens$featureID == features_sel$ID[3]),]
points_D = featCens[which(featCens$featureID == features_sel$ID[4]),]

# points_A = points_A[which(points_A$retentionTime %in% points_B$retentionTime),]
# points_B = points_B[which(points_B$retentionTime %in% points_A$retentionTime),]
# points_A = points_A[which(points_A$retentionTime %in% points_B$retentionTime),]
# points_B = points_B[which(points_B$retentionTime %in% points_A$retentionTime),]

plot_base = ggplot()+
  geom_point(aes(points_A$retentionTime, points_A$area), colour = "red2")+
  geom_point(aes(points_B$retentionTime, points_B$area), colour = "lightblue")+
  geom_point(aes(points_C$retentionTime, points_C$area), colour = "green")+
  geom_point(aes(points_D$retentionTime, points_D$area), colour = "orange")

# scale vectors
points_A$area = points_A$area / sqrt(sum(points_A$area^2))
points_B$area = points_B$area / sqrt(sum(points_B$area^2))
points_C$area = points_C$area / sqrt(sum(points_C$area^2))
points_D$area = points_D$area / sqrt(sum(points_D$area^2))

plot_scaled = ggplot()+
  geom_point(aes(points_A$retentionTime, points_A$area), colour = "red2")+
  geom_point(aes(points_B$retentionTime, points_B$area), colour = "lightblue")+
  geom_point(aes(points_C$retentionTime, points_C$area), colour = "green")+
  geom_point(aes(points_D$retentionTime, points_D$area), colour = "orange")

cumSD = 0
for (i in 1:length(points_A$retentionTime)) {
  cumSD[i] = sd(c(points_A$area[i], points_B$area[i]))
}

cumSD = sort(cumSD)
cumSD = cumsum(cumSD)
cumSD = cumSD / cumSD[length(cumSD)]

ggplot()+
  geom_point(aes(x = points_A$retentionTime, y = cumSD), colour = "red2")+
  geom_abline(slope = 1, intercept = 0, linewidth = 2)

# this process will not work for components of size two, which are the majority of all components
```

```{r}
scaleScore = function() {}

# calculate the tanimoto over multiple regressions by taking the minimum and maximum values for a given x
comp_sel = compRegs[149,] # this must be -1 for 0-indexed arrays
features_sel = features[which(features$CompID == comp_sel$compID),]
points = featCens[which(featCens$featureID %in% features_sel$ID),]

# find min and max for every RT
minima = 0
maxima = 0
rts = 0
for (index in 1:length(points$retentionTime)) {
  rt = points$retentionTime[index]
  if (rt %in% rts){
    next
  }
  rts[index] = rt
  tmpPoints = points[which(points$retentionTime == rt),]
  tmp_min = min(tmpPoints$area)
  tmp_max = max(tmpPoints$area)
  if (tmp_min == tmp_max){
    minima[index] = NA
    maxima[index] = NA
  } else {
    minima[index] = tmp_min
    maxima[index] = tmp_max
  }
}

minima = na.omit(minima)
maxima = na.omit(maxima)

squares_ab = minima * maxima
minima = minima ^2
maxima = maxima ^2

score = sum(squares_ab) / (sum(minima) + sum(maxima) - sum(squares_ab))
```


```{r}
# the tanimoto index might not be a good idea to use here, how should scaling be handled?

test_a = points[which(points$featureID == 23376),]
test_b = points[which(points$featureID == 23378),][-1,]

feat_a = features[23376,]
feat_b = features[23378,]
# test_b = test_a + rnorm(17, 5000, 10)

test_a = test_a$area / feat_a$area
test_b = test_b$area / feat_b$area

asq_real = 0
bsq_real = 0
ab_real = 0
for (i in 1:17) {
  asq_real = asq_real + test_a[i]^2
  bsq_real = bsq_real + test_b[i]^2
  ab_real = ab_real + test_a[i] * test_b[i]
}
score_real_sqs = ab_real / (asq_real + bsq_real - ab_real)

exclude = 0
asum = 0
bsum = 0
for (i in 1:17) {
  asum = asum + test_a[i]
  bsum = bsum + test_b[i]
  exclude = exclude + min(test_a[i], test_b[i])
}

score_real_jc = exclude / (asum + bsum - exclude)

score_real_sqs
score_real_jc
################################################################################

diff_a = 0
diff_b = 0

for (i in 2:17) {
  diff_a[i] = test_a$area[i] / test_a$area[i-1]
  diff_b[i] = test_b$area[i] / test_b$area[i-1]
}
diff_a
diff_b

# randomised control for test viability
con_a_tmp = rnorm(17, 100, 5)
con_b_tmp = rnorm(17, 10, 6)
con_a = 0
con_b = 0
for (i in 2:17) {
  con_a[i] = con_a_tmp[i] / con_a_tmp[i-1]
  con_b[i] = con_b_tmp[i] / con_b_tmp[i-1]
}

asq_real = 0
bsq_real = 0
ab_real = 0
asq_fake = 0
bsq_fake = 0
ab_fake = 0
for (i in 1:16) {
  asq_real = asq_real + diff_a[i]^2
  bsq_real = bsq_real + diff_b[i]^2
  ab_real = ab_real + diff_a[i] * diff_b[i]
  asq_fake = asq_fake + con_a[i]^2
  bsq_fake = bsq_fake + con_b[i]^2
  ab_fake = ab_fake + con_a[i] * con_b[i]
}

score_real = ab_real / (asq_real + bsq_real - ab_real)
score_fake = ab_fake / (asq_fake + bsq_fake - ab_fake)


 tibble(features[11806,])
```

