---
title: "Untitled"
author: "Daniel HÃ¶hn"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

```{r load data}
# dir = "C:/Users/unisys/Documents/Studium/qAlgorithms/build/"
dir = "C:/Users/unisys/Documents/Studium/Masterarbeit/"
polarity = "_positive_"
name = "SP_DDA_P7"
# name = "warburg_testdata"
# name = "210229_C1_S1_W_MI_2_pos"
# name = "20240805_AA_DK_02_Ibu_Kali_0_20240808125236"
file = str_c(dir, name, polarity)
file_feat = str_c(file, "features.csv")
file_bins = str_c(file, "bins.csv")
file_featCens = str_c(file, "featCen.csv")
file_compRegs = str_c(file, "components.csv")

SPP10 = read_csv(file_feat)
featCens = read_csv(file_featCens)
compRegs = read_csv(file_compRegs)
bins = read_csv(file_bins)
```

```{r included standards}
SPP = features
M_H = 1.0078

standards <- data.frame(
  standard = c("Hydroxycotinine", "Hydroxycotinine-d3", "Anatabine", "Cotinine", 
               "Cotinine-d3", "Anabasine", "Anabasine-d4", "Vanillylmandelic acid", 
               "Homovanillic acid", "Atrazine desisopropyl", "Bisphenol A", 
               "Bisphenol A-d16", "3-phenoxybenzoic acid"),
  rt_expect = c(2.43, 2.45, 4.24, 5.39, 5.38, 5.61, 
                      5.61, 6.14, 7.67, 7.77, 11.69, 11.55, 13.55),
  mz = c(192.089877630, 195.108707868, 160.100048391, 176.094963011, 179.113793248, 
           162.115698455, 166.140805439, 198.05282342, 182.05790880, 173.0468230, 
           228.115029749, 244.215457684, 214.062994177),
  stringsAsFactors = FALSE
)
standards$mz = standards$mz + M_H
standards$rt_expect = standards$rt_expect * 60

standards$rt_expect[1] = 151.5
standards$rt_expect[1] = 149

candidates_rt = function(standards, SPP){
  # reduce the feature list to relevant RTs
  rts = standards$rt_expect
  retFrame = SPP[1,]
  for (rt in rts) {
    lower = SPP$retentionTime > rt - 30
    upper = SPP$retentionTime < rt + 30
    retFrame = rbind(retFrame, SPP[which(lower & upper),])
  }
  retFrame = retFrame[-1,]
  retFrame = retFrame[!duplicated(retFrame$ID), ]
  # same for mass
  retFrame2 = SPP[1,]
  for (mz in standards$mz) {
    tolerance = mz * 2 * 0.00001 # 20 ppm
    lower = retFrame$mz > mz - tolerance
    upper = retFrame$mz < mz + tolerance
    retFrame2 = rbind(retFrame2, retFrame[which(lower & upper),])
  }
  retFrame2 = retFrame2[-1,]
  retFrame2 = retFrame2[!duplicated(retFrame2$ID), ]
  return(retFrame2)
}

closestMass = function(standards, SPP){
  # return the standards dataframe with a feature ID added and the observed values for RT and mz
  standards$ID = 0
  standards$rt_observed = 0
  standards$mz_real = 0
  standards$area = 0
  standards$component = -1
  standards$ID = 0
  for (comp in 1:length(standards$mz)) {
    mz = standards$mz[comp]
    rt = standards$rt_expect[comp]
    # rt tolerance of 40 seconds
    rmv = !(SPP$retentionTime > rt - 3 & SPP$retentionTime < rt + 3)
    add = 10^100 * rmv
    lowestMZ = (SPP$mz - mz)^2 + add
    minPos = which(lowestMZ == min(lowestMZ))[1] # this is a bad error solution, rework for paper
    tolerance = mz * 2 * 0.00001 # 20 ppm
    
    standards$ID[comp] = SPP$ID[minPos]
    if(SPP$mz[minPos] > mz - tolerance & SPP$mz[minPos] < mz + tolerance){
      standards$rt_observed[comp] = SPP$retentionTime[minPos]
      standards$mz_real[comp] = SPP$mz[minPos]
      standards$component[comp] = SPP$CompID[minPos]
      standards$area[comp] = SPP$area[minPos]
    }
  }
  return(standards)
}

all1 = test[1,]
all2 = test[1,]

for(i in 1:10){
  name = paste0("SPP",i)
  SPP = get(name)
test = closestMass(standards,SPP)
selection = c(0,0)
selection[1] = test$ID[1]
selection[2] = test$ID[2]

all1 = rbind(all1, test[1,])
all2 = rbind(all2, test[2,])

feat = features[which(features$ID %in% selection),]
   points = featCens[which(featCens$featureID %in% feat$ID),]
   for (i in 1:length(feat$ID)) {
     points$mz[which(points$featureID == feat$ID[i])] = round(feat$mz[i], 4)
   }

 
print(ggplot(test)+
     geom_point(aes(retentionTime, area, colour = as.factor(round(mz,2))))+             
  labs(x = "Retention Time [s]",                       
       y = "Area",
       colour = "m/z")+                  
    theme_minimal()+ 
  theme(text = element_text(size = 12),
        axis.title.x = element_text(face = "bold"),
        axis.title.y = element_text(face = "bold")) 
     
)
}
all1 = all1[-1,]
all2 = all2[-1,]

ggplot(compDIffs)+
  geom_point(aes(log10(range), dqsp), colour = "blue", size = 2)+
  labs(x = "Intensity Difference (log10)",                       
       y = "DQS")+ 
  theme_minimal()+ 
  theme(text = element_text(size = 12),
        axis.title.x = element_text(face = "bold"),
        axis.title.y = element_text(face = "bold")) 
```

I am relatively certain that the previous efforts with the normalised intensity profiles only seemed promising due to
the differences between absolute intensities being minimal. Scaling itself seems to be the biggest problem in solving
shape similarity through the tanimoto index / score 

```{r}
compID = 1
mzDiffCount = 0
intensityDiffGreaterTwo = 0

for (compID in 1:max(features$CompID)) {
  feat = features[which(features$CompID == compID),]
   points = featCens[which(featCens$featureID %in% feat$ID),]
  for (i in 1:length(feat$ID)) {
     points$mz[which(points$featureID == feat$ID[i])] = feat$mz[i]
  }
  print(
   ggplot(points)+
     geom_point(aes(retentionTime, height, colour = as.factor(mz)))+
     labs(caption = paste0("compID: ", compID))
   )

}
```


```{r}
compID = 247
mzDiffCount = 0
intensityDiffGreaterTwo = 0

for (compID in 1:max(features$CompID)) {
  feat = features[which(features$CompID == compID),]
  points = featCens[which(featCens$featureID %in% feat$ID),]
  for (i in 1:length(feat$ID)) {
     points$mz[which(points$featureID == feat$ID[i])] = feat$mz[i]
  }
  MZdiffs = c(outer(feat$mz, feat$mz, `-`))
  IntDiffs = c(outer(feat$area, feat$area, `-`))
  mzDiffCount[compID] = sum(abs(MZdiffs - 1.003355) < mean(feat$mzUncertainty))
  intensityDiffGreaterTwo[compID] = sum(IntDiffs > max(feat$area) / 2)
}
sum(mzDiffCount != 0)
sum(intensityDiffGreaterTwo != 0)
sel = which(mzDiffCount != 0)
for (compID in sel) {
  feat = features[which(features$CompID == compID),]
   points = featCens[which(featCens$featureID %in% feat$ID),]
   for (i in 1:length(feat$ID)) {
     points$mz[which(points$featureID == feat$ID[i])] = feat$mz[i]
   }

   print(
   ggplot(points)+
     geom_point(aes(retentionTime, height, colour = as.factor(mz)))+
     labs(caption = paste0("compID: ", compID))
   )
}

# check number of true duplicate retention times
RTmatch = 0
for (compID in 1:max(features$CompID)) {
  feat = features[which(features$CompID == compID),]
  points = featCens[which(featCens$featureID %in% feat$ID),]
  RTmatch[compID] = (length(points$mz) - length(unique(points$retentionTime))) / length(points$mz)
}
```

```{r plot distribution of components}
ggplot(features)+
  geom_boxplot(aes(retentionTime, CompID != 0))

# a very high portion of features in the last few minutes is in components
cdplot(as.factor(CompID != 0) ~ retentionTime, data = features)
plot(density(features$retentionTime))

feat = features[which(features$CompID == 343),]
   points = featCens[which(featCens$featureID %in% feat$ID),]
   for (i in 1:length(feat$ID)) {
     points$mz[which(points$featureID == feat$ID[i])] = feat$mz[i]
   }

   print(
   ggplot(points)+
     geom_point(aes(retentionTime, height, colour = as.factor(mz)))+
     labs(caption = paste0("compID: ", compID))
   )
```

```{r gini coeff}
# basic idea: take all points in the two or more same-length, interpolated vectors and normalise them to
# a vector length of 1. Then, take the standard deviation for every discrete x value. If all vectors follow
# the same profile with a normally distributed difference, the contribution of every point to the total error
# should be roughly equivalent. This corresponds to the cumulative error function x = y. The area of the 
# empirical cumulative error (triangle) divided by the ideal area yields a measure of equivalence that does
# not heavily weight the intensity uncertainty

features_sel = features[which(features$CompID == 702),]
comp_sel = compRegs[702,] # this must be -1 for 0-indexed arrays
points_A = featCens[which(featCens$featureID == features_sel$ID[1]),]
points_B = featCens[which(featCens$featureID == features_sel$ID[2]),]
points_C = featCens[which(featCens$featureID == features_sel$ID[3]),]
points_D = featCens[which(featCens$featureID == features_sel$ID[4]),]

# points_A = points_A[which(points_A$retentionTime %in% points_B$retentionTime),]
# points_B = points_B[which(points_B$retentionTime %in% points_A$retentionTime),]
# points_A = points_A[which(points_A$retentionTime %in% points_B$retentionTime),]
# points_B = points_B[which(points_B$retentionTime %in% points_A$retentionTime),]

plot_base = ggplot()+
  geom_point(aes(points_A$retentionTime, points_A$area), colour = "red2")+
  geom_point(aes(points_B$retentionTime, points_B$area), colour = "lightblue")+
  geom_point(aes(points_C$retentionTime, points_C$area), colour = "green")+
  geom_point(aes(points_D$retentionTime, points_D$area), colour = "orange")

# scale vectors
points_A$area = points_A$area / sqrt(sum(points_A$area^2))
points_B$area = points_B$area / sqrt(sum(points_B$area^2))
points_C$area = points_C$area / sqrt(sum(points_C$area^2))
points_D$area = points_D$area / sqrt(sum(points_D$area^2))

plot_scaled = ggplot()+
  geom_point(aes(points_A$retentionTime, points_A$area), colour = "red2")+
  geom_point(aes(points_B$retentionTime, points_B$area), colour = "lightblue")+
  geom_point(aes(points_C$retentionTime, points_C$area), colour = "green")+
  geom_point(aes(points_D$retentionTime, points_D$area), colour = "orange")

cumSD = 0
for (i in 1:length(points_A$retentionTime)) {
  cumSD[i] = sd(c(points_A$area[i], points_B$area[i]))
}

cumSD = sort(cumSD)
cumSD = cumsum(cumSD)
cumSD = cumSD / cumSD[length(cumSD)]

ggplot()+
  geom_point(aes(x = points_A$retentionTime, y = cumSD), colour = "red2")+
  geom_abline(slope = 1, intercept = 0, linewidth = 2)

# this process will not work for components of size two, which are the majority of all components
```

```{r}
scaleScore = function() {}

# calculate the tanimoto over multiple regressions by taking the minimum and maximum values for a given x
comp_sel = compRegs[149,] # this must be -1 for 0-indexed arrays
features_sel = features[which(features$CompID == comp_sel$compID),]
points = featCens[which(featCens$featureID %in% features_sel$ID),]

# find min and max for every RT
minima = 0
maxima = 0
rts = 0
for (index in 1:length(points$retentionTime)) {
  rt = points$retentionTime[index]
  if (rt %in% rts){
    next
  }
  rts[index] = rt
  tmpPoints = points[which(points$retentionTime == rt),]
  tmp_min = min(tmpPoints$area)
  tmp_max = max(tmpPoints$area)
  if (tmp_min == tmp_max){
    minima[index] = NA
    maxima[index] = NA
  } else {
    minima[index] = tmp_min
    maxima[index] = tmp_max
  }
}

minima = na.omit(minima)
maxima = na.omit(maxima)

squares_ab = minima * maxima
minima = minima ^2
maxima = maxima ^2

score = sum(squares_ab) / (sum(minima) + sum(maxima) - sum(squares_ab))
```


```{r}
# the tanimoto index might not be a good idea to use here, how should scaling be handled?

test_a = points[which(points$featureID == 23376),]
test_b = points[which(points$featureID == 23378),][-1,]

feat_a = features[23376,]
feat_b = features[23378,]
# test_b = test_a + rnorm(17, 5000, 10)

test_a = test_a$area / feat_a$area
test_b = test_b$area / feat_b$area

asq_real = 0
bsq_real = 0
ab_real = 0
for (i in 1:17) {
  asq_real = asq_real + test_a[i]^2
  bsq_real = bsq_real + test_b[i]^2
  ab_real = ab_real + test_a[i] * test_b[i]
}
score_real_sqs = ab_real / (asq_real + bsq_real - ab_real)

exclude = 0
asum = 0
bsum = 0
for (i in 1:17) {
  asum = asum + test_a[i]
  bsum = bsum + test_b[i]
  exclude = exclude + min(test_a[i], test_b[i])
}

score_real_jc = exclude / (asum + bsum - exclude)

score_real_sqs
score_real_jc
################################################################################

diff_a = 0
diff_b = 0

for (i in 2:17) {
  diff_a[i] = test_a$area[i] / test_a$area[i-1]
  diff_b[i] = test_b$area[i] / test_b$area[i-1]
}
diff_a
diff_b

# randomised control for test viability
con_a_tmp = rnorm(17, 100, 5)
con_b_tmp = rnorm(17, 10, 6)
con_a = 0
con_b = 0
for (i in 2:17) {
  con_a[i] = con_a_tmp[i] / con_a_tmp[i-1]
  con_b[i] = con_b_tmp[i] / con_b_tmp[i-1]
}

asq_real = 0
bsq_real = 0
ab_real = 0
asq_fake = 0
bsq_fake = 0
ab_fake = 0
for (i in 1:16) {
  asq_real = asq_real + diff_a[i]^2
  bsq_real = bsq_real + diff_b[i]^2
  ab_real = ab_real + diff_a[i] * diff_b[i]
  asq_fake = asq_fake + con_a[i]^2
  bsq_fake = bsq_fake + con_b[i]^2
  ab_fake = ab_fake + con_a[i] * con_b[i]
}

score_real = ab_real / (asq_real + bsq_real - ab_real)
score_fake = ab_fake / (asq_fake + bsq_fake - ab_fake)


 tibble(features[11806,])
```

